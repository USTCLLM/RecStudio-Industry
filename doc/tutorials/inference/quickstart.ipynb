{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Tutorials\n",
    "In an online recommendation system, handling a single request typically involves the following steps:\n",
    "- **Receiving the request header**: The request header includes the user ID and context-specific features (e.g., location and timestamp of the request).\n",
    "- **Obtaining the Candidate Item Set**: At each stage, the recommendation model receives the candidate item set from the previous stage (for the retrieval model, it is the entire item pool).\n",
    "- **Retrieving Features**: At each stage, the system retrieves user- and item-related features required by the recommendation model based on the user ID and candidate item IDs. To enable fast access, user and item features are stored in a cache database (e.g., Redis) in a key-value format.\n",
    "- **Sorting the Candidate Item Set**: At each stage, the recommendation model ranks the candidate items using the retrieved features and selects the top-k items to pass to the next stage (for the final stage, the top-k items are directly presented to the user)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Features in Cache Database\n",
    "### Defining message in protobuf\n",
    "To reduce the cache size occupied by features, Protobuf is used to serialize the features before storing them in the cache database. To use Protobuf,  message data structures must first be defined.\n",
    "\n",
    "In the .proto file, the user and item message data structures are defined. For example, in recflow.proto:\n",
    "\n",
    "Each feature of user and item is treated as a field of the message structure."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# the version of protobuf\n",
    "\n",
    "syntax = \"proto3\"; \n",
    "\n",
    "package example;\n",
    "\n",
    "message Item {\n",
    "  int64 video_id = 1;\n",
    "  int64 author_id = 2;\n",
    "  int64 category_level_two = 3;\n",
    "  int64 upload_type = 4;\n",
    "  int64 upload_timestamp = 5;\n",
    "  int64 category_level_one = 6;\n",
    "  int64 request_timestamp = 7; \n",
    "}\n",
    "\n",
    "message UserTimestamp {\n",
    "  int64 request_id = 1;          \n",
    "  int64 user_id = 2;             \n",
    "  int64 request_timestamp = 3;    \n",
    "  int64 device_id = 4;           \n",
    "  int32 age = 5;                  \n",
    "  int64 gender = 6;              \n",
    "  int64 province = 7;\n",
    "  repeated Item seq_effective_50 = 8;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, generate Python code from the .proto file using protoc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# create proto\n",
    "protoc --python_out=. ./inference/feature_insert/protos/recflow.proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Features into Redis Database\n",
    "When storing user-side or item-side features in a Redis database, the process typically involves several steps:\n",
    "\n",
    "​\t1.\tCreate a message object.\n",
    "\n",
    "​\t2.\tAssign values to each field of the message object.\n",
    "\n",
    "​\t3.\tSerialize the message object.\n",
    "\n",
    "​\t4.\tStore the serialized message object in the Redis database. The key is usually set as {dataset_name}:{object_name}:{object_primary_key}.\n",
    "\n",
    "An example of inserting features into the Redis database using recflow is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import *\n",
    "\n",
    "import recflow_pb2\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Item\n",
    "test_video_info = pd.read_feather('./inference/feature_data/recflow/realshow_test_video_info.feather')\n",
    "for row in tqdm(test_video_info.itertuples(), total=len(test_video_info)):\n",
    "\n",
    "    # 0. Create a message object\n",
    "    item = recflow_pb2.Item()\n",
    "    item.video_id = getattr(row, 'video_id')\n",
    "    item.author_id = getattr(row, 'author_id')\n",
    "    item.category_level_two = getattr(row, '_3')\n",
    "    item.upload_type = getattr(row, 'upload_type')\n",
    "    item.upload_timestamp = getattr(row, 'upload_timestamp')\n",
    "    item.category_level_one = getattr(row, 'category_level_one')\n",
    "    \n",
    "    # 1. Serialize the Protobuf object into binary data\n",
    "    serialized_data = item.SerializeToString()\n",
    "\n",
    "    # 2. Store the compressed data in Redis\n",
    "    r.set(f\"recflow:item:{item.video_id}\", serialized_data)\n",
    "    \n",
    "\n",
    "print(\"Item features are stored in Redis.\")\n",
    "\n",
    "# User\n",
    "test_user_info = np.load('./inference/feature_data/recflow/test_user_info.npz')['arr_0']\n",
    "for row in tqdm(test_user_info):\n",
    "\n",
    "    # 0. Create a message object \n",
    "    user_timestamp = recflow_pb2.UserTimestamp()\n",
    "    user_timestamp.request_id = row[0]\n",
    "    user_timestamp.user_id = row[1]\n",
    "    user_timestamp.request_timestamp = row[2]\n",
    "    user_timestamp.device_id = row[3]\n",
    "    user_timestamp.age = row[4]\n",
    "    user_timestamp.gender = row[5]\n",
    "    user_timestamp.province = row[6]\n",
    "    \n",
    "    for behavior in np.split(test_user_info[0][7:], len(test_user_info[0][7:]) // 6):\n",
    "        item = user_timestamp.seq_effective_50.add()\n",
    "        item.video_id = behavior[0]\n",
    "        item.author_id = behavior[1]\n",
    "        item.category_level_two = behavior[2]\n",
    "        item.category_level_one = behavior[3]\n",
    "        item.upload_type = behavior[4]\n",
    "        item.request_timestamp = behavior[5]\n",
    "\n",
    "    # 1. Serialize the Protobuf object into binary data\n",
    "    serialized_data = user_timestamp.SerializeToString()\n",
    "\n",
    "    # 2. Store the compressed data in Redis\n",
    "    r.set(f\"recflow:user_timestamp:{row[1]}_{row[2]}\", serialized_data)\n",
    "\n",
    "print(\"UserTimestamp features are stored in Redis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate cache configuration file `feature_cache_config.yaml`\n",
    "\n",
    "To enable the use of features stored in the cache, we need to generate a configuration file `feature_cache_config.yaml` for each dataset.\n",
    "\n",
    "Taking Recflow as an example:\n",
    "\n",
    "The `host`, `port`, and `db` fields specify details of Redis database. `features`  specifies the storage details for each feature. Within `features`, `key_temp` represents the key template for the feature in Redis database, where the content inside {} is replaced with specific item or user information, and `field` specifies the attribute name of the feature in the message object. `key_temp2proto` maps each key template to the corresponding message class name, which is used to create message objects."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "host: localhost\n",
    "port: 6379\n",
    "db: 0\n",
    "features:\n",
    "  video_id:\n",
    "    key_temp: recflow:item:{video_id}\n",
    "    field: video_id\n",
    "  author_id:\n",
    "    key_temp: recflow:item:{video_id}\n",
    "    field: author_id\n",
    "  category_level_two:\n",
    "    key_temp: recflow:item:{video_id}\n",
    "    field: category_level_two\n",
    "...\n",
    "  request_id:\n",
    "    key_temp: recflow:user_timestamp:{user_id}_{request_timestamp}\n",
    "    field: request_id\n",
    "  user_id:\n",
    "    key_temp: recflow:user_timestamp:{user_id}_{request_timestamp}\n",
    "    field: user_id\n",
    "  request_timestamp:\n",
    "    key_temp: recflow:user_timestamp:{user_id}_{request_timestamp}\n",
    "    field: request_timestamp\n",
    "...\n",
    "  seq_effective_50:\n",
    "    key_temp: recflow:user_timestamp:{user_id}_{request_timestamp}\n",
    "    field: seq_effective_50\n",
    "key_temp2proto:\n",
    "  recflow:item:{video_id}:\n",
    "    class_name: Item\n",
    "    module_path: ./inference/feature_insert/protos/recflow_pb2.py\n",
    "  recflow:user_timestamp:{user_id}_{request_timestamp}:\n",
    "    class_name: UserTimestamp\n",
    "    module_path: ./inference/feature_insert/protos/recflow_pb2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running ./inference/feature_insert/recflow_script/run.sh completes the three steps mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "### InferenceEngine\n",
    "\n",
    "[InferenceEngine](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/inference_engine.py#L21) class can be initialized to perform the inference process, which primarily consists of the following steps:\n",
    "\n",
    "1. Converting a checkpoint of the recommendation model to an `onnxruntime.InferenceSession`.\n",
    "2.\tPerforming batch inference.\n",
    "3.\tOutputting the top-k candidate items.\n",
    "\n",
    "We can initialize the InferenceEngine class and perform batch inference as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from inference.inference.inference_engine import InferenceEngine\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--infer_config_path\", type=str, required=True, help=\"Inference config file\")  \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.infer_config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    rank_inference_engine = InferenceEngine(config)\n",
    "    ranker_outputs = rank_inference_engine.batch_inference()\n",
    "    rank_inference_engine.save_output_topk(ranker_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further reference, check the Recflow ranking stage inference implementation in [rank_stage.py](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/recflow_script/rank_stage.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a checkpoint to an InferenceSession\n",
    "\n",
    "The [get_ort_session()](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/inference_engine.py#L59) function in InferenceEngine is used to convert the recommendation model’s checkpoint into an onnxruntime.InferenceSession. The InferenceEngine class invokes the get_ort_session() function within its \\_\\_init\\_\\_() method to obtain the inference session, which is then used for inference. Based on the inference session, operator optimization can be considered during the inference process.  For reference, you can check the [get_ort_session()](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/recflow_script/rank_stage.py#L32) function in the ranking stage of the Recflow dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch inference\n",
    "\n",
    "Batch inference is implemented in the [InferenceEngine.batch_inference()](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/inference_engine.py#L62) function. It includes the following steps:\n",
    "\n",
    "​\t1.\t**Obtain the candidate item set**: The candidate items can be passed directly to the `batch_inference()` function or index from `candidates_df`.\n",
    "\n",
    "​\t2.\t**Fetch user and context features**: The [get_user_context_features()](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/inference_engine.py#L102) function is used to obtain user and context features.\n",
    "\n",
    "​\t3.\t**Fetch candidate item features**: The [get_candidates_features()](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/inference_engine.py#L145) function is used to obtain features for the candidate items.\n",
    "\n",
    "​\t4.\t**Feed the features to the inference session**: Feed the features for inference to the inferenece session and return the top-k results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customization\n",
    "\n",
    "Users can extend the `InferenceEngine` class and override its methods to implement custom functionality:\n",
    "\n",
    "​\t•\t[get_ort_session()](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/inference_engine.py#L59): Override this method to modify the model conversion process.\n",
    "\n",
    "​\t•\t[batch_inference()](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/inference_engine.py#L62): Override this method to customize the batch inference process.\n",
    "\n",
    "​\t•\t[get_user_context_features()](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/inference_engine.py#L102): Override this method to customize how user and context features are fetched.\n",
    "\n",
    "​\t•\t[get_candidates_features()](https://gitee.com/recstudio-team/rec-studio-industry/blob/master/inference/inference/inference_engine.py#L145): Override this method to customize how candidate item features are fetched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "To initialize the `InferenceEngine` class, an inference configuration file `infer_config.yaml` is required. Using the Recflow dataset as an example, the meanings of each parameter in the configuration file are as follows:\n",
    "\n",
    "​\t•\t`stage`: Specifies the stage of the inference process.\n",
    "\n",
    "​\t•\t`model_ckpt_path`: Specifies the path to the recommendation model checkpoint.\n",
    "\n",
    "​\t•\t`feature_cache_config_path`: Specifies the path to the feature cache configuration.\n",
    "\n",
    "​\t•\t`inference_dataset_path`: Specifies the path to the dataset used for inference.\n",
    "\n",
    "​\t•\t`candidates_path`: Specifies the path to the candidate set file.\n",
    "\n",
    "​\t•\t`output_topk`: Specifies the number of top-k items to return from the candidate set.\n",
    "\n",
    "​\t•\t`request_features`: Features used to construct the request_key, which serves as the key to index the candidate items set.\n",
    "\n",
    "​\t•\t`output_save_path`: Specifies the path to save the inference results.\n",
    "\n",
    "​\t•\t`infer_batch_size`: Specifies the batch size for inference.\n",
    "\n",
    "​\t•\t`infer_device`: Specifies the GPU index to be used by the inference model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "stage: rank \n",
    "model_ckpt_path: saved_model_demo/ranker_best_ckpt\n",
    "feature_cache_config_path: inference/feature_insert/feature_cache_configs/recflow_feature_cache_config.yaml\n",
    "inference_dataset_path: inference/inference_data/recflow/recflow_infer_data.feather\n",
    "candidates_path: inference/inference_data/recflow/candidates_demo.feather\n",
    "output_topk: 10\n",
    "request_features: ['user_id', 'request_timestamp']\n",
    "output_save_path: inference/inference_data/recflow/ranker_outputs.feather\n",
    "infer_batch_size: 128\n",
    "infer_device: 0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
